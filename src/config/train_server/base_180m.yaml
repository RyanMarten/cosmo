

setting:
  output_dir: "/experiments/cosmo/"
  src_dir: "./src"
  add_time_stamp: True # if False, when restart, will know where to resume from (suitable for very long training)
  resume_from_checkpoint: True
  prompt_template_name: "cosmo"



data_setting:
  local_prefix: "/storage/" # !!!!! will set to "" if use azfuse
  train:
    img_txt_path: >
      /dataset/data_selection_v0/data_comp_1b/chunk0/{000000000..000000336}.tar;
      /dataset/data_selection_v0/data_comp_1b/chunk1/{000000000..000000020}.tar;
      /dataset/data_selection_v0/data_comp_1b/chunk2/{000000000..000000655}.tar;
      /dataset/data_selection_v0/data_comp_1b/chunk3/{000000000..000000382}.tar;
      /dataset/data_selection_v0/data_comp_1b/chunk4/{000000000..000000933}.tar;
      /dataset/data_selection_v0/data_comp_1b/chunk5/{000000000..000000425}.tar;
    inter_img_txt_path: >
      /dataset/mmc4_ff_wds/chunk0/{000000000..000000006}.tar;
      /dataset/mmc4_ff_wds/chunk1/{000000000..000000117}.tar;
      /dataset/mmc4_ff_wds/chunk2/{000000000..000000993}.tar;
      /dataset/mmc4_ff_wds/chunk3/{000000000..000000816}.tar;
      /dataset/mmc4_ff_wds/chunk4/{000000000..000000159}.tar;
      /dataset/mmc4_ff_wds/chunk5/{000000000..000000388}.tar;
      /dataset/mmc4_ff_wds/chunk6/{000000000..000000128}.tar;
      /dataset/obelics_wds/chunk0/{000000000..000000750}.tar;
      /dataset/obelics_wds/chunk1/{000000000..000000600}.tar;
      /dataset/obelics_wds/chunk2/{000000000..000000500}.tar;
      /dataset/obelics_wds/chunk3/{000000000..000000500}.tar;
      /dataset/obelics_wds/chunk4/{000000000..000000500}.tar;
      /dataset/obelics_wds/chunk5/{000000000..000000500}.tar;
      /dataset/obelics_wds/chunk6/{000000000..000000700}.tar
    vid_txt_wds_path: >

    vid_txt_tsv_path: >

    inter_vid_txt_wds_path: >

    inter_vid_txt_tsv_path: >

    instr_path:
      ""
    
  eval:
    img_txt_path: >
      /dataset/laion400m_wds_caption/{00998..00999}.tar;
    inter_img_txt_path: >
      /dataset/mmc4/core_ff_w_clean_w_rule1_chunk_wds_caption/chunk0/{000000900..000000978}.tar;
      /dataset/mmc4/core_ff_w_clean_w_rule1_chunk_wds_caption/chunk1/{000000900..000000972}.tar;
      /dataset/mmc4/core_ff_w_clean_w_rule1_chunk_wds_caption/chunk2/{000001000..000001021}.tar;
      /dataset/mmc4/core_ff_w_clean_w_rule1_chunk_wds_caption/chunk3/{000001000..000001582}.tar
    vid_txt_wds_path: >

    vid_txt_tsv_path: >

    inter_vid_txt_wds_path: >

    inter_vid_txt_tsv_path: >

    instr_path:
      ""
    
dataset_params:
  use_azfuse: False # <----------------!!!!!!! Modfify this line!!!!!!!!!---------------->
  split_data_by_node: True # !!!!!
  sampling_strategy: "min" # round_robin, min, max, 
  upload_model_to_blob: True # !!!!!
  img_txt:
    use_img_txt: True
    clean_data_use_strategy: "noisy_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy    
    tar_pre_cache: False # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 0 # if tar_pre_cache is True, only download 10% of tar files
  inter_img_txt:
    use_inter_img_txt: True
    MIN_KB: 10 # reject image smaller than 10KB
    MAX_IMAGE_PIXELS: 1000000000
    MAX_NUM_TOKENS_MMC4: 128  # max length of text token, 256 in flamingo (with larger memory)
    MAX_NUM_TOKENS_OBELICS: 128  # max length of text token, 256 in flamingo (with larger memory)
    MAX_NUM_IMAGES_MMC4: 3  # images num in each interlevel image_text pairs, 5 in flamingo
    MAX_NUM_IMAGES_OBELICS: 3  # 50%+ obelics only have 1 images, notice ehty must be the same
    SIM_THRESHOLD_MMC4: 0.24
    SIM_THRESHOLD_OBELICS: 0.24
    TINY_IMAGE_SIZE_THRESHOLD: 1
    N_CHANNELS: 3
    INTERLEAVED_IMAGE_SIZE: 224
    clean_data_use_strategy: "noisy_only"    #low_simlarity clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    interlevel_text_coherence: False # If True, sample MAX_NUM_TOKENS token from adjacent text; If False, sample K images and their matched text
    balanced_sampling: False # If true, sample K images and K texts possibly, for obelics
    tar_pre_cache: False # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 0.25 # if tar_pre_cache is True, only download 10% of tar files
  vid_txt:
    use_vid_txt: True
    VIDEO_IMAGE_SIZE: 224
    VIDEO_FRAMES: 3
    clean_data_use_strategy: "noisy_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    MAX_SAMPLES: -1 # 500000, use 0.5M video-text pair at most, -1 for use all dataset
    tar_pre_cache: False # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    read_mode: "wds" # tsv, wds  # <----------------!!!!!!! Modfify this line!!!!!!!!!---------------->
    pre_cache_ratio: 0.25 # if tar_pre_cache is True, only download 10% of tar files
  inter_vid_txt:
    use_inter_vid_txt: True
    VIDEO_IMAGE_SIZE: 224
    VIDEO_SAMPLED_CLIPS: 2
    MAX_SAMPLES: -1 # 500000, use 0.5M video-text pair at most, -1 for use all dataset
    MAX_NUM_TOKENS: 128  # max length of text token, 256 in flamingo (with larger memory)
    VIDEO_FRAMES: 3
    read_mode: "wds" # tsv, wds  tsv downloaded from blob cache
    clean_data_use_strategy: "noisy_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    tar_pre_cache: False # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 0.25 # if tar_pre_cache is True, only download 10% of tar files

model_params:
  vision_encoder:
    vision_encoder_name: "clip" # clip/sam/sparseformer
    vision_encoder_arch: "ViT-L-14"
    tuning: False
    vision_encoder_pretrained: "openai"
    ckpt_path: ""
    cache_dir: "/tmp/pretrained_models"
    custom_augment: True # custom augmentation is much stronger than the default one
  lang_model:
    name: "opt-iml-max-1.3b"
    lang_model_path: "/tmp/pretrained_models/opt-iml-max-1.3b"
    tokenizer_path: "/tmp/pretrained_models/opt-iml-max-1.3b"
    dim: 512
    num_tokens: 512
    unimodal_depth: 12
    interval_layer: 2
    use_memory_layer: False # if True, add 1 memory layer after unimodal layer, but require faiss library
    
  multimodality_model:
    latent_dim: 512 # latent space for contrastive learning
    use_contrastive_loss: True
    contrastive_temperature: 0.2
    contrastive_loss_weight: 1.0
    contrastive_gather_way: "single_gpu" # "single_node", "all_nodes", "single_gpu"
    cross_attention_compress_ratio: 2 # compress self-attention and feedforward layer in CrossAttention module
    only_attend_immediate_media: True # if True, only attend to immediate media in CrossAttention module
    # set to False for obelics
    qv_norm: False # if True, normalize q and v in CrossAttention module

training_params:
  optim: "adamw_hf" # replace by deepspeed config file
  micro_batch_size: 96
  workers: 4 # workers per GPU
  num_epochs: 5
  lr_scheduler_type: "cosine"
  learning_rate: 0.0005
  cutoff_len: 256
  warmup_steps: 500 # prior than warmup_ratio
  warmup_ratio: 0.03
  logging_steps: 200 # do not set it too small, otherwise it will slow down the training (due to the overhead of logging)
  save_steps: 3000
  save_total_limit: 10
  # the following for validation
  eval_steps: 500
  eval: True # if evaluate
  max_eval_batches: 500
  data_weights:
    img_txt: 2
    vid_txt: 1
    inter_img_txt: 2
    inter_vid_txt: 2
  exception_handling: False # if True, the training will not stop when exception occurs
  ignore_data_skip: True # if False, when resume from checkpoint, need to iteration all dataloader until reach the last iteration (exception)
  data_resampling: False # when use tsv, will set to True auto

lora_params:
  lora: False
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj"]

wandb_params:
  wandb: True # True for use, False for not use
  wandb_project: "cosmo"
  wandb_run_name: "cosmoe"
  wandb_watch: "all"
  wandb_log_model: ""
  wandb_dir: "/experiments/cosmo/wandb_logs/"
